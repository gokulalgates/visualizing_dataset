{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import threading\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import pickle\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import sklearn.metrics as metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression#create a new logistic regression model\n",
    "from sklearn.ensemble import RandomForestClassifier#create a new random forest classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier#create new a knn model\n",
    "\n",
    "import os\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    " \n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Descriptors,Crippen\n",
    " \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "data =pd.read_csv(\"../NURA_v1.0.0.csv\")\n",
    "\n",
    "#Add some new columns\n",
    "data['Mol'] = data['Canonical SMILES'].apply(Chem.MolFromSmiles)\n",
    "num_mols = len(data)\n",
    "\n",
    "#Create X and y\n",
    "#Convert to Numpy arrays\n",
    "#y = data['LABEL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy\n",
    "\n",
    "smiles =data['Mol']\n",
    "\n",
    "#fps = [AllChem.GetMorganFingerprintAsBitVect(m, 4) for m in smiles]\n",
    "#np_fps = numpy.asarray(fps)\n",
    "\n",
    "radius=3\n",
    "nBits=1024\n",
    "\n",
    "ECFP6 = [AllChem.GetMorganFingerprintAsBitVect(x,radius=radius, nBits=nBits) for x in data['Mol']]\n",
    "\n",
    "\n",
    "ecfp6_name = [f'Bit_{i}' for i in range(nBits)]\n",
    "ecfp6_bits = [list(l) for l in ECFP6]\n",
    "df_morgan = pd.DataFrame(ecfp6_bits, index = data.NURA_ID, columns=ecfp6_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cols = ['AGO_PR', 'ANT_PR', 'BIN_PR', 'AGO_PXR',\n",
    "       'ANT_PXR', 'BIN_PXR', 'AGO_RXR', 'ANT_RXR', 'BIN_RXR', 'AGO_GR',\n",
    "       'ANT_GR', 'BIN_GR', 'AGO_AR', 'ANT_AR', 'BIN_AR', 'AGO_ERA', 'ANT_ERA',\n",
    "       'BIN_ERA', 'AGO_ERB', 'ANT_ERB', 'BIN_ERB', 'AGO_FXR', 'ANT_FXR',\n",
    "       'BIN_FXR', 'AGO_PPARD', 'ANT_PPARD', 'BIN_PPARD', 'AGO_PPARG',\n",
    "       'ANT_PPARG', 'BIN_PPARG', 'AGO_PPARA', 'ANT_PPARA', 'BIN_PPARA']\n",
    "\n",
    "# Dimension of Train and Test set \n",
    "#print(\"Dimension of Train set\",X_train.shape)\n",
    "#print(\"Dimension of Test set\",X_test.shape,\"\\n\")\n",
    "\n",
    "\n",
    "#data[cols]= data[cols].apply(LabelEncoder().fit_transform)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/pandas/core/frame.py:4304: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n.a.      8106\n",
       "inact.    5063\n",
       "act.      2078\n",
       "Name: PR, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_PR = data[['NURA_ID','AGO_PR','Canonical SMILES']]\n",
    "ANT_PR = data[['NURA_ID','ANT_PR','Canonical SMILES']]\n",
    "BIN_PR = data[['NURA_ID','BIN_PR','Canonical SMILES']]\n",
    "AGO_PR.rename(columns={'AGO_PR':'PR'},inplace=True)\n",
    "ANT_PR.rename(columns={'ANT_PR':'PR'},inplace=True)\n",
    "BIN_PR.rename(columns={'BIN_PR':'PR'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_PR,ANT_PR])\n",
    "PR = pd.concat([temp,BIN_PR])\n",
    "PR.loc[(PR.PR == 'w.act.'),'PR']='act.'\n",
    "PR.sort_values(by = 'PR') \n",
    "\n",
    "PR_NODUPLICATES = PR.sort_values('PR', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "PR_NODUPLICATES.PR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_NODUPLICATES_act = PR_NODUPLICATES[PR_NODUPLICATES['PR'] != \"n.a.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "PR_NODUPLICATES_act.sort_values(['PR'],  ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inact.    5063\n",
       "act.      2078\n",
       "Name: PR, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PR_NODUPLICATES_act['PR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tmap as tm\n",
    "from mhfp.encoder import MHFPEncoder\n",
    "from faerun import Faerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in PR_NODUPLICATES_act['Canonical SMILES']]\n",
    "PR_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in PR_NODUPLICATES_act['mols']]\n",
    "PR_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "PR_NODUPLICATES_act[\"SmilesID\"] = PR_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + PR_NODUPLICATES_act[\"NURA_ID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Prepare custom color map\n",
    "tab10 = plt.get_cmap(\"tab20\").colors\n",
    "colors_gray = [ tab10[0],(0.95, 0.95, 0.95), tab10[5], tab10[2], tab10[3], tab10[4], tab10[1],tab10[6],tab10[7],tab10[8],tab10[9],tab10[10],tab10[11],tab10[12],tab10[13]]\n",
    "#colors_gray = [ tab10[0],(0.95, 0.95, 0.95), tab10[2], tab10[4]]\n",
    "custom_cm_gray = LinearSegmentedColormap.from_list(\"custom_cm_gray\", colors_gray, N=len(colors_gray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(PR_NODUPLICATES_act['PR'])\n",
    "PR_TRANSFORM = le.transform(PR_NODUPLICATES_act['PR'])\n",
    "print(PR_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(PR_TRANSFORM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [PR_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\")]],\n",
    "    series_title=[\"PR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d147b2b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      7869\n",
       "inact.    5232\n",
       "act.      2143\n",
       "inc.         3\n",
       "Name: GR, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_GR = data[['NURA_ID','AGO_GR','Canonical SMILES']]\n",
    "ANT_GR = data[['NURA_ID','ANT_GR','Canonical SMILES']]\n",
    "BIN_GR = data[['NURA_ID','BIN_GR','Canonical SMILES']]\n",
    "AGO_GR.rename(columns={'AGO_GR':'GR'},inplace=True)\n",
    "ANT_GR.rename(columns={'ANT_GR':'GR'},inplace=True)\n",
    "BIN_GR.rename(columns={'BIN_GR':'GR'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_GR,ANT_GR])\n",
    "GR = pd.concat([temp,BIN_GR])\n",
    "GR.loc[(GR.GR == 'w.act.'),'GR']='act.'\n",
    "GR.sort_values(by = 'GR') \n",
    "\n",
    "GR_NODUPLICATES = GR.sort_values('GR', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "GR_NODUPLICATES.GR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "GR_NODUPLICATES_act = GR_NODUPLICATES[GR_NODUPLICATES['GR'] != \"n.a.\"]\n",
    "GR_NODUPLICATES_act.sort_values(['GR'],  ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in GR_NODUPLICATES_act['Canonical SMILES']]\n",
    "GR_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in GR_NODUPLICATES_act['mols']]\n",
    "GR_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['inc.',\n",
       " 'inc.',\n",
       " 'inc.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " 'inact.',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GR_NODUPLICATES_act[\"SmilesID\"] = GR_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + GR_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(GR_NODUPLICATES_act['GR'])\n",
    "GR_TRANSFORM = le.transform(GR_NODUPLICATES_act['GR'])\n",
    "print(GR_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(GR_TRANSFORM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [GR_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"GR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d14c68b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      7832\n",
       "inact.    5179\n",
       "act.      2217\n",
       "inc.        19\n",
       "Name: AR, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_AR = data[['NURA_ID','AGO_AR','Canonical SMILES']]\n",
    "ANT_AR = data[['NURA_ID','ANT_AR','Canonical SMILES']]\n",
    "BIN_AR = data[['NURA_ID','BIN_AR','Canonical SMILES']]\n",
    "AGO_AR.rename(columns={'AGO_AR':'AR'},inplace=True)\n",
    "ANT_AR.rename(columns={'ANT_AR':'AR'},inplace=True)\n",
    "BIN_AR.rename(columns={'BIN_AR':'AR'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_AR,ANT_AR])\n",
    "AR = pd.concat([temp,BIN_AR])\n",
    "AR.loc[(AR.AR == 'w.act.'),'AR']='act.'\n",
    "AR.sort_values(by = 'AR') \n",
    "\n",
    "AR_NODUPLICATES = AR.sort_values('AR', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "AR_NODUPLICATES.AR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d0c5622b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AR_NODUPLICATES_act = AR_NODUPLICATES[AR_NODUPLICATES['AR'] != \"n.a.\"]\n",
    "AR_NODUPLICATES_act.sort_values(['AR'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in AR_NODUPLICATES_act['Canonical SMILES']]\n",
    "AR_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in AR_NODUPLICATES_act['mols']]\n",
    "AR_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "AR_NODUPLICATES_act[\"SmilesID\"] = AR_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + AR_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(AR_NODUPLICATES_act['AR'])\n",
    "AR_TRANSFORM = le.transform(AR_NODUPLICATES_act['AR'])\n",
    "print(AR_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(AR_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [AR_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"AR\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      7943\n",
       "inact.    4956\n",
       "act.      2327\n",
       "inc.        21\n",
       "Name: ERA, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_ERA = data[['NURA_ID','AGO_ERA','Canonical SMILES']]\n",
    "ANT_ERA = data[['NURA_ID','ANT_ERA','Canonical SMILES']]\n",
    "BIN_ERA = data[['NURA_ID','BIN_ERA','Canonical SMILES']]\n",
    "AGO_ERA.rename(columns={'AGO_ERA':'ERA'},inplace=True)\n",
    "ANT_ERA.rename(columns={'ANT_ERA':'ERA'},inplace=True)\n",
    "BIN_ERA.rename(columns={'BIN_ERA':'ERA'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_ERA,ANT_ERA])\n",
    "ERA = pd.concat([temp,BIN_ERA])\n",
    "ERA.loc[(ERA.ERA == 'w.act.'),'ERA']='act.'\n",
    "ERA.sort_values(by = 'ERA') \n",
    "\n",
    "ERA_NODUPLICATES = ERA.sort_values('ERA', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "ERA_NODUPLICATES.ERA.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0cfc7235f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ERA_NODUPLICATES_act = ERA_NODUPLICATES[ERA_NODUPLICATES['ERA'] != \"n.a.\"]\n",
    "ERA_NODUPLICATES_act.sort_values(['ERA'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in ERA_NODUPLICATES_act['Canonical SMILES']]\n",
    "ERA_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in ERA_NODUPLICATES_act['mols']]\n",
    "ERA_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "ERA_NODUPLICATES_act[\"SmilesID\"] = ERA_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + ERA_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(ERA_NODUPLICATES_act['ERA'])\n",
    "ERA_TRANSFORM = le.transform(ERA_NODUPLICATES_act['ERA'])\n",
    "print(ERA_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(ERA_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [ERA_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"ERA\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      8127\n",
       "inact.    5563\n",
       "act.      1552\n",
       "inc.         5\n",
       "Name: ERB, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_ERB = data[['NURA_ID','AGO_ERB','Canonical SMILES']]\n",
    "ANT_ERB = data[['NURA_ID','ANT_ERB','Canonical SMILES']]\n",
    "BIN_ERB = data[['NURA_ID','BIN_ERB','Canonical SMILES']]\n",
    "AGO_ERB.rename(columns={'AGO_ERB':'ERB'},inplace=True)\n",
    "ANT_ERB.rename(columns={'ANT_ERB':'ERB'},inplace=True)\n",
    "BIN_ERB.rename(columns={'BIN_ERB':'ERB'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_ERB,ANT_ERB])\n",
    "ERB = pd.concat([temp,BIN_ERB])\n",
    "ERB.loc[(ERB.ERB == 'w.act.'),'ERB']='act.'\n",
    "ERB.sort_values(by = 'ERB') \n",
    "\n",
    "ERB_NODUPLICATES = ERB.sort_values('ERB', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "ERB_NODUPLICATES.ERB.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d14c1d978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ERB_NODUPLICATES_act = ERB_NODUPLICATES[ERB_NODUPLICATES['ERB'] != \"n.a.\"]\n",
    "ERB_NODUPLICATES_act.sort_values(['ERB'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in ERB_NODUPLICATES_act['Canonical SMILES']]\n",
    "ERB_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in ERB_NODUPLICATES_act['mols']]\n",
    "ERB_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "ERB_NODUPLICATES_act[\"SmilesID\"] = ERB_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + ERB_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(ERB_NODUPLICATES_act['ERB'])\n",
    "ERB_TRANSFORM = le.transform(ERB_NODUPLICATES_act['ERB'])\n",
    "print(ERB_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(ERB_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [ERB_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"ERB\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      9120\n",
       "inact.    5276\n",
       "act.       837\n",
       "inc.        14\n",
       "Name: FXR, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_FXR = data[['NURA_ID','AGO_FXR','Canonical SMILES']]\n",
    "ANT_FXR = data[['NURA_ID','ANT_FXR','Canonical SMILES']]\n",
    "BIN_FXR = data[['NURA_ID','BIN_FXR','Canonical SMILES']]\n",
    "AGO_FXR.rename(columns={'AGO_FXR':'FXR'},inplace=True)\n",
    "ANT_FXR.rename(columns={'ANT_FXR':'FXR'},inplace=True)\n",
    "BIN_FXR.rename(columns={'BIN_FXR':'FXR'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_FXR,ANT_FXR])\n",
    "FXR = pd.concat([temp,BIN_FXR])\n",
    "FXR.loc[(FXR.FXR == 'w.act.'),'FXR']='act.'\n",
    "FXR.sort_values(by = 'FXR') \n",
    "\n",
    "FXR_NODUPLICATES = FXR.sort_values('FXR', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "FXR_NODUPLICATES.FXR.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0cfc5f9128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FXR_NODUPLICATES_act = FXR_NODUPLICATES[FXR_NODUPLICATES['FXR'] != \"n.a.\"]\n",
    "FXR_NODUPLICATES_act.sort_values(['FXR'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in FXR_NODUPLICATES_act['Canonical SMILES']]\n",
    "FXR_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in FXR_NODUPLICATES_act['mols']]\n",
    "FXR_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "FXR_NODUPLICATES_act[\"SmilesID\"] = FXR_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + FXR_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(FXR_NODUPLICATES_act['FXR'])\n",
    "FXR_TRANSFORM = le.transform(FXR_NODUPLICATES_act['FXR'])\n",
    "print(FXR_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(FXR_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [FXR_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"FXR\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      8587\n",
       "inact.    5745\n",
       "act.       848\n",
       "inc.        67\n",
       "Name: PPARD, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_PPARD = data[['NURA_ID','AGO_PPARD','Canonical SMILES']]\n",
    "ANT_PPARD = data[['NURA_ID','ANT_PPARD','Canonical SMILES']]\n",
    "BIN_PPARD = data[['NURA_ID','BIN_PPARD','Canonical SMILES']]\n",
    "AGO_PPARD.rename(columns={'AGO_PPARD':'PPARD'},inplace=True)\n",
    "ANT_PPARD.rename(columns={'ANT_PPARD':'PPARD'},inplace=True)\n",
    "BIN_PPARD.rename(columns={'BIN_PPARD':'PPARD'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_PPARD,ANT_PPARD])\n",
    "PPARD = pd.concat([temp,BIN_PPARD])\n",
    "PPARD.loc[(PPARD.PPARD == 'w.act.'),'PPARD']='act.'\n",
    "PPARD.sort_values(by = 'PPARD') \n",
    "\n",
    "PPARD_NODUPLICATES = PPARD.sort_values('PPARD', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "PPARD_NODUPLICATES.PPARD.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d14c33c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PPARD_NODUPLICATES_act = PPARD_NODUPLICATES[PPARD_NODUPLICATES['PPARD'] != \"n.a.\"]\n",
    "PPARD_NODUPLICATES_act.sort_values(['PPARD'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in PPARD_NODUPLICATES_act['Canonical SMILES']]\n",
    "PPARD_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in PPARD_NODUPLICATES_act['mols']]\n",
    "PPARD_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "PPARD_NODUPLICATES_act[\"SmilesID\"] = PPARD_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + PPARD_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(PPARD_NODUPLICATES_act['PPARD'])\n",
    "PPARD_TRANSFORM = le.transform(PPARD_NODUPLICATES_act['PPARD'])\n",
    "print(PPARD_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(PPARD_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [PPARD_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"PPARD\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      7644\n",
       "inact.    5469\n",
       "act.      2118\n",
       "inc.        16\n",
       "Name: PPARG, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_PPARG = data[['NURA_ID','AGO_PPARG','Canonical SMILES']]\n",
    "ANT_PPARG = data[['NURA_ID','ANT_PPARG','Canonical SMILES']]\n",
    "BIN_PPARG = data[['NURA_ID','BIN_PPARG','Canonical SMILES']]\n",
    "AGO_PPARG.rename(columns={'AGO_PPARG':'PPARG'},inplace=True)\n",
    "ANT_PPARG.rename(columns={'ANT_PPARG':'PPARG'},inplace=True)\n",
    "BIN_PPARG.rename(columns={'BIN_PPARG':'PPARG'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_PPARG,ANT_PPARG])\n",
    "PPARG = pd.concat([temp,BIN_PPARG])\n",
    "PPARG.loc[(PPARG.PPARG == 'w.act.'),'PPARG']='act.'\n",
    "PPARG.sort_values(by = 'PPARG') \n",
    "\n",
    "PPARG_NODUPLICATES = PPARG.sort_values('PPARG', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "PPARG_NODUPLICATES.PPARG.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0cf7fcf898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PPARG_NODUPLICATES_act = PPARG_NODUPLICATES[PPARG_NODUPLICATES['PPARG'] != \"n.a.\"]\n",
    "PPARG_NODUPLICATES_act.sort_values(['PPARG'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in PPARG_NODUPLICATES_act['Canonical SMILES']]\n",
    "PPARG_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in PPARG_NODUPLICATES_act['mols']]\n",
    "PPARG_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "PPARG_NODUPLICATES_act[\"SmilesID\"] = PPARG_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + PPARG_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(PPARG_NODUPLICATES_act['PPARG'])\n",
    "PPARG_TRANSFORM = le.transform(PPARG_NODUPLICATES_act['PPARG'])\n",
    "print(PPARG_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(PPARG_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [PPARG_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"PPARG\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      13800\n",
       "act.       1364\n",
       "inc.         68\n",
       "inact.       15\n",
       "Name: PPARA, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_PPARA = data[['NURA_ID','AGO_PPARA','Canonical SMILES']]\n",
    "ANT_PPARA = data[['NURA_ID','ANT_PPARA','Canonical SMILES']]\n",
    "BIN_PPARA = data[['NURA_ID','BIN_PPARA','Canonical SMILES']]\n",
    "AGO_PPARA.rename(columns={'AGO_PPARA':'PPARA'},inplace=True)\n",
    "ANT_PPARA.rename(columns={'ANT_PPARA':'PPARA'},inplace=True)\n",
    "BIN_PPARA.rename(columns={'BIN_PPARA':'PPARA'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_PPARA,ANT_PPARA])\n",
    "PPARA = pd.concat([temp,BIN_PPARA])\n",
    "PPARA.loc[(PPARA.PPARA == 'w.act.'),'PPARA']='act.'\n",
    "PPARA.sort_values(by = 'PPARA') \n",
    "\n",
    "PPARA_NODUPLICATES = PPARA.sort_values('PPARA', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "PPARA_NODUPLICATES.PPARA.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d14c29fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PPARA_NODUPLICATES_act = PPARA_NODUPLICATES[PPARA_NODUPLICATES['PPARA'] != \"n.a.\"]\n",
    "PPARA_NODUPLICATES_act.sort_values(['PPARA'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in PPARA_NODUPLICATES_act['Canonical SMILES']]\n",
    "PPARA_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in PPARA_NODUPLICATES_act['mols']]\n",
    "PPARA_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "PPARA_NODUPLICATES_act[\"SmilesID\"] = PPARA_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + PPARA_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(PPARA_NODUPLICATES_act['PPARA'])\n",
    "PPARA_TRANSFORM = le.transform(PPARA_NODUPLICATES_act['PPARA'])\n",
    "print(PPARA_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(PPARA_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [PPARA_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"PPARA\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n.a.      9666\n",
       "inact.    4569\n",
       "act.      1008\n",
       "inc.         4\n",
       "Name: RXR, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGO_RXR = data[['NURA_ID','AGO_RXR','Canonical SMILES']]\n",
    "ANT_RXR = data[['NURA_ID','ANT_RXR','Canonical SMILES']]\n",
    "BIN_RXR = data[['NURA_ID','BIN_RXR','Canonical SMILES']]\n",
    "AGO_RXR.rename(columns={'AGO_RXR':'RXR'},inplace=True)\n",
    "ANT_RXR.rename(columns={'ANT_RXR':'RXR'},inplace=True)\n",
    "BIN_RXR.rename(columns={'BIN_RXR':'RXR'},inplace=True)\n",
    "\n",
    "temp = pd.concat([AGO_RXR,ANT_RXR])\n",
    "RXR = pd.concat([temp,BIN_RXR])\n",
    "RXR.loc[(RXR.RXR == 'w.act.'),'RXR']='act.'\n",
    "RXR.sort_values(by = 'RXR') \n",
    "\n",
    "RXR_NODUPLICATES = RXR.sort_values('RXR', ascending=True).drop_duplicates(subset=['NURA_ID'])\n",
    "RXR_NODUPLICATES.RXR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/singam2/anaconda3/envs/playground/lib/python3.6/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750\"\n",
       "            src=\"./index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0d14c51f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='./index.html' target='_blank'>./index.html</a><br>"
      ],
      "text/plain": [
       "/mnt/d/UCBerkeley/Toxicity_machine_learning/Toxicity_machine_learning/FINAL/pubchem/index.html"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RXR_NODUPLICATES_act = RXR_NODUPLICATES[RXR_NODUPLICATES['RXR'] != \"n.a.\"]\n",
    "RXR_NODUPLICATES_act.sort_values(['RXR'],  ascending=False,inplace=True)\n",
    "\n",
    "bits = 1024\n",
    "from rdkit.Chem import MACCSkeys\n",
    "#fps = [MACCSkeys.GenMACCSKeys(x) for x in ms]\n",
    "mols = [Chem.MolFromSmiles(s) for s in RXR_NODUPLICATES_act['Canonical SMILES']]\n",
    "RXR_NODUPLICATES_act['mols'] = mols\n",
    "mol_noH = [Chem.RemoveHs(mol) for mol in RXR_NODUPLICATES_act['mols']]\n",
    "RXR_NODUPLICATES_act['mol_noH'] = mol_noH\n",
    "ECFP4_fps = [AllChem.GetMorganFingerprintAsBitVect(x,3,bits) for x in mol_noH]\n",
    "#ECFP4_fps = [MACCSkeys.GenMACCSKeys(x) for x in mol_noH]\n",
    "ecfp4_lists = [tm.VectorUchar(list(fp)) for fp in ECFP4_fps]\n",
    "\n",
    "\n",
    "# The number of permutations used by the MinHashing algorithm\n",
    "#perm = 512\n",
    "\n",
    "# Initializing the MHFP encoder with 512 permutations\n",
    "enc = tm.Minhash(bits)\n",
    "\n",
    "# Initialize the LSH Forest\n",
    "lf_ecfp4 = tm.LSHForest(bits)\n",
    "\n",
    "# Create MHFP fingerprints from SMILES\n",
    "# The fingerprint vectors have to be of the tm.VectorUint data type\n",
    "lf_ecfp4.batch_add(enc.batch_from_binary_array(ecfp4_lists))\n",
    "lf_ecfp4.index()\n",
    "# Get the coordinates\n",
    "x, y, s, t, _ = tm.layout_from_lsh_forest(lf_ecfp4)\n",
    "\n",
    "\n",
    "RXR_NODUPLICATES_act[\"SmilesID\"] = RXR_NODUPLICATES_act[\"Canonical SMILES\"] + '__' + RXR_NODUPLICATES_act[\"NURA_ID\"]\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(RXR_NODUPLICATES_act['RXR'])\n",
    "RXR_TRANSFORM = le.transform(RXR_NODUPLICATES_act['RXR'])\n",
    "print(RXR_TRANSFORM)\n",
    "\n",
    "list(le.inverse_transform(RXR_TRANSFORM))\n",
    "\n",
    "\n",
    "faerun = Faerun(clear_color=\"#ffffff\", view=\"front\", coords=False)\n",
    "\n",
    "faerun.add_scatter(\n",
    "    \"Activity\",\n",
    "    {\"x\": x, \"y\": y, \"c\": [RXR_TRANSFORM], \"labels\": PR_NODUPLICATES_act[\"SmilesID\"]},\n",
    "    point_scale=10,\n",
    "    colormap=custom_cm_gray,\n",
    "    shader=\"smoothCircle\",\n",
    "    has_legend=True,\n",
    "    #categorical=True,\n",
    "    categorical=True,\n",
    "    legend_labels=[[(0, \"Active\"), (1, \"Inactive\"),(2, \"Inconclusive\")]],\n",
    "    series_title=[\"RXR\"])\n",
    "\n",
    "\n",
    "faerun.add_tree(\"Assay_tree\", {\"from\": s, \"to\": t}, point_helper=\"Activity\",color=\"#666666\")\n",
    "\n",
    "\n",
    "# Choose the \"smiles\" template to display structure on hover\n",
    "faerun.plot(template=\"smiles\", notebook_height=750)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Bit_0, Bit_1, Bit_2, Bit_3, Bit_4, Bit_5, Bit_6, Bit_7, Bit_8, Bit_9, Bit_10, Bit_11, Bit_12, Bit_13, Bit_14, Bit_15, Bit_16, Bit_17, Bit_18, Bit_19, Bit_20, Bit_21, Bit_22, Bit_23, Bit_24, Bit_25, Bit_26, Bit_27, Bit_28, Bit_29, Bit_30, Bit_31, Bit_32, Bit_33, Bit_34, Bit_35, Bit_36, Bit_37, Bit_38, Bit_39, Bit_40, Bit_41, Bit_42, Bit_43, Bit_44, Bit_45, Bit_46, Bit_47, Bit_48, Bit_49, Bit_50, Bit_51, Bit_52, Bit_53, Bit_54, Bit_55, Bit_56, Bit_57, Bit_58, Bit_59, Bit_60, Bit_61, Bit_62, Bit_63, Bit_64, Bit_65, Bit_66, Bit_67, Bit_68, Bit_69, Bit_70, Bit_71, Bit_72, Bit_73, Bit_74, Bit_75, Bit_76, Bit_77, Bit_78, Bit_79, Bit_80, Bit_81, Bit_82, Bit_83, Bit_84, Bit_85, Bit_86, Bit_87, Bit_88, Bit_89, Bit_90, Bit_91, Bit_92, Bit_93, Bit_94, Bit_95, Bit_96, Bit_97, Bit_98, Bit_99, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 1024 columns]\n"
     ]
    }
   ],
   "source": [
    "df_morgan.filter(regex='Pubchem').isnull().values.any()\n",
    "is_NaN = df_morgan.isnull()\n",
    "\n",
    "row_has_NaN = is_NaN.any(axis=1)\n",
    "\n",
    "rows_with_NaN = df_morgan[row_has_NaN]\n",
    "\n",
    "\n",
    "print(rows_with_NaN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RXR\n",
      "n.a.      9666\n",
      "inact.    4569\n",
      "act.      1008\n",
      "inc.         4\n",
      "Name: label, dtype: int64\n",
      "(5577, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-8d7ab237b936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"n.a.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mstd_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Canonical SMILES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std_smiles'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd_smiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-8d7ab237b936>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"n.a.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mstd_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_smiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Canonical SMILES'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std_smiles'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd_smiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/playground/lib/python3.6/site-packages/molvs/standardize.py\u001b[0m in \u001b[0;36mstandardize_smiles\u001b[0;34m(smiles)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# Skip sanitize as standardize does this anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolFromSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMolToSmiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misomericSmiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/playground/lib/python3.6/site-packages/molvs/standardize.py\u001b[0m in \u001b[0;36mstandardize\u001b[0;34m(self, mol)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSanitizeMol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRemoveHs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect_metals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreionize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/playground/lib/python3.6/site-packages/molvs/metal.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mol)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m\"\"\"Calling a MetalDisconnector instance like a function is the same as calling its disconnect(mol) method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/playground/lib/python3.6/site-packages/molvs/metal.py\u001b[0m in \u001b[0;36mdisconnect\u001b[0;34m(self, mol)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Remove bonds that match SMARTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msmarts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metal_nof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metal_non\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSubstructMatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmarts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mrwmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRWMol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0morders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/playground/lib/python3.6/site-packages/rdkit/Chem/Draw/IPythonConsole.py\u001b[0m in \u001b[0;36m_GetSubstructMatches\u001b[0;34m(mol, query, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_GetSubstructMatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__GetSubstructMatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m   \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sssAtoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhighlightSubstructs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from molvs import standardize_smiles\n",
    "#DATAFRAMES = [GR_NODUPLICATES]\n",
    "#COLUMS = ['GR']\n",
    "DATAFRAMES = [RXR_NODUPLICATES,PR_NODUPLICATES,GR_NODUPLICATES,AR_NODUPLICATES,ERA_NODUPLICATES,ERB_NODUPLICATES,FXR_NODUPLICATES,PPARD_NODUPLICATES,PPARG_NODUPLICATES]#,PPARA_NODUPLICATES]\n",
    "COLUMS = ['RXR','PR','GR','AR','ERA','ERB','FXR','PPARD','PPARG']#,'PPARA']\n",
    "a_table = pd.DataFrame(columns = ['Sets','Acc','precision','Recall','MCC','TP','TN','FP','FN'])\n",
    "a_table['Sets'] = [\"KNN\",\"AdaBoostClassifier\",\"RandomForestClassifier\",\"SVM\"]\n",
    "j = 0\n",
    "k = 0\n",
    "i=0\n",
    "for dataframe in DATAFRAMES:\n",
    "    print(COLUMS[i])\n",
    "    filename_smiles = COLUMS[i] + \".smi\"\n",
    "\n",
    "    X = dataframe[['NURA_ID','Canonical SMILES']]\n",
    "    X['label'] =  dataframe[COLUMS[i]]\n",
    "\n",
    "    \n",
    "    print(X.label.value_counts())\n",
    "    X= X[X['label'] != \"inc.\"]\n",
    "    X= X[X['label'] != \"n.a.\"]\n",
    "    print(X.shape)\n",
    "    std_smiles = [standardize_smiles(x) for x in X['Canonical SMILES']]\n",
    "    X['std_smiles'] = std_smiles\n",
    "    \n",
    "    X['Mol'] = X['Canonical SMILES'].apply(Chem.MolFromSmiles)\n",
    "    \n",
    "    np.savetxt(filename_smiles, X[['Canonical SMILES', 'NURA_ID' ]].values, fmt='%s')\n",
    "    filename_csv = COLUMS[i] + \"_pubchem.csv\"\n",
    "    from subprocess import Popen\n",
    "\n",
    "    proc = Popen([\"java\", \"-jar\", \"PaDEL-Descriptor.jar\", \"-dir\", filename_smiles, \"-file\", filename_csv, \"-fingerprints\", \"-retainorder\", \"-maxruntime\", \"-1\"])\n",
    "    proc.wait()  # block execution until the sub-process terminate.\n",
    "\n",
    "\n",
    "    #radius=3\n",
    "    #nBits=1024\n",
    "    #ECFP6 = [AllChem.GetMorganFingerprintAsBitVect(x,radius=radius, nBits=nBits) for x in X['Mol']]\n",
    "    #ecfp6_name = [f'Bit_{i}' for i in range(nBits)]\n",
    "    #ecfp6_bits = [list(l) for l in ECFP6]\n",
    "    #df_morgan = pd.DataFrame(ecfp6_bits, index = X.NURA_ID, columns=ecfp6_name)\n",
    "    df_morgan = pd.read_csv(filename_csv)\n",
    "    \n",
    "    df_morgan['label'] = X['label'].values\n",
    "    print(df_morgan.label.value_counts())\n",
    "    \n",
    "    df_morgan['SMILES'] = X['Canonical SMILES'].values\n",
    "    df_morgan = df_morgan.dropna()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(df_morgan, df_morgan['label'], test_size=0.2, random_state=50223251)\n",
    "    \n",
    "    print(Y_test.value_counts())\n",
    "    print(Y_train.value_counts())\n",
    "    #filename = COLUMS[i] + \".xlsx\"\n",
    "    #print(filename)\n",
    "    #with pd.ExcelWriter(filename) as writer:  \n",
    "     #   X_train.to_excel(writer, sheet_name='X_train')\n",
    "     #   X_test.to_excel(writer,sheet_name='X_test')\n",
    "     #   Y_train.to_excel(writer, sheet_name='Y_train')\n",
    "      #  Y_test.to_excel(writer, sheet_name='Y_test')\n",
    "    X_train =X_train.filter(regex='Pubchem').values\n",
    "    X_test =X_test.filter(regex='Pubchem').values\n",
    "    Y_train = Y_train\n",
    "    Y_test = Y_test\n",
    "    from sklearn import preprocessing\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "    #encoder.fit(X['label'])\n",
    "    #Y_val = encoder.transform(Y)\n",
    "    # encoding train labels \n",
    "    encoder.fit(Y_train)\n",
    "    Y_train = encoder.transform(Y_train)\n",
    "    \n",
    "    # encoding test labels \n",
    "    #encoder.fit(Y_test)\n",
    "    Y_test = encoder.transform(Y_test)\n",
    "    \n",
    "    y_true = Y_test\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    filename = COLUMS[i] + \"classes.npy\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    numpy.save(filename, encoder.classes_)\n",
    "    \n",
    "    \n",
    "    print(\"##################- Running KNN - ######################\")\n",
    "    knn = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "    params_knn = {'n_neighbors': np.arange(1, 50),\n",
    "                  'algorithm' :['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                  'weights': ['uniform','distance'],\n",
    "                  'metric': ['euclidean','manhattan']}#use gridsearch to test all values for n_neighbors\n",
    "    knn_gs = GridSearchCV(knn, params_knn, cv=skf, n_jobs=-1, scoring=\"f1_micro\")#fit model to training data\n",
    "    knn_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    knn_best = knn_gs.best_estimator_#check best n_neigbors value\n",
    "    print(knn_gs.best_params_)\n",
    "    print('knn: {}'.format(knn_best.score(X_test, Y_test)))\n",
    "    y_pred = knn_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "    expected = Y_test\n",
    "    predicted = y_pred\n",
    "    recall = recall_score(expected, predicted)\n",
    "    precision = precision_score(expected, predicted)\n",
    "    MCC = metrics.matthews_corrcoef(expected, predicted)\n",
    "    a_table.iloc[k,1] = metrics.accuracy_score(y_pred,Y_test)\n",
    "    a_table.iloc[k,2] = MCC\n",
    "    a_table.iloc[k,3] = precision\n",
    "    a_table.iloc[k,4] = recall\n",
    "    k=k+1\n",
    "    print(\"##################- Running AdaBoostClassifier - ######################\")\n",
    "\n",
    "    adaboost = AdaBoostClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "    params_adaboost = {'n_estimators': np.arange(1, 500),}#use gridsearch to test all values for n_neighbors\n",
    "    adaboost_gs = GridSearchCV(adaboost, params_adaboost, cv=skf, n_jobs=-1,scoring= \"f1_micro\")#fit model to training data\n",
    "    adaboost_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    adaboost_best = adaboost_gs.best_estimator_#check best n_neigbors value\n",
    "    print(adaboost_gs.best_params_)\n",
    "    print('adaboost: {}'.format(adaboost_best.score(X_test, Y_test)))\n",
    "    y_pred = adaboost_gs.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "    a_table.iloc[k,1] = metrics.accuracy_score(y_pred,Y_test)\n",
    "    a_table.iloc[k,2] = MCC\n",
    "    a_table.iloc[k,3] = precision\n",
    "    a_table.iloc[k,4] = recall\n",
    "    k=k+1\n",
    "    print(\"##################- Running RandomForestClassifier - ######################\")\n",
    "\n",
    "    rf = RandomForestClassifier(class_weight='balanced')#create a dictionary of all values we want to test for n_estimators\n",
    "    params_rf = {'n_estimators': [50, 100, 200,300,500, 1000], \n",
    "                 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                 'min_samples_split': [3, 5, 10],\n",
    "                 'min_samples_leaf': [8, 10, 12],\n",
    "                 'max_depth': [80, 90, 100, 110],\n",
    "                 'max_features': [3, 5, 10, 20],\n",
    "                 'bootstrap': [True],}#use gridsearch to test all values for n_estimators\n",
    "    rf_gs = GridSearchCV(rf, params_rf, cv=skf,n_jobs=-1,scoring= \"f1_micro\")#fit model to training data\n",
    "    rf_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    rf_best = rf_gs.best_estimator_#check best n_estimators value\n",
    "    print(rf_gs.best_params_)\n",
    "    print('rf: {}'.format(rf_best.score(X_test, Y_test)))\n",
    "    y_pred = rf_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "    C_range=[1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]\n",
    "    gamma_range=[1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,1.e-01, 1.e+00, ]\n",
    "    \n",
    "    a_table.iloc[k,1] = metrics.accuracy_score(y_pred,Y_test)\n",
    "    a_table.iloc[k,2] = MCC\n",
    "    a_table.iloc[k,3] = precision\n",
    "    a_table.iloc[k,4] = recall\n",
    "    k=k+1\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma_range,'C': C_range},\n",
    "                        {'kernel': ['poly'], 'gamma': gamma_range,'C': C_range},\n",
    "                        {'kernel': ['sigmoid'],'gamma': gamma_range, 'C': C_range}]\n",
    "    print(\"##################- Running SVM - ######################\")\n",
    "\n",
    "    svm_gs = GridSearchCV(SVC(class_weight='balanced',probability=True), tuned_parameters,iid=True, cv=skf,scoring=\"f1_micro\",n_jobs=-1)\n",
    "    svm_gs.fit(X_train, Y_train)\n",
    "    svm_best = svm_gs.best_estimator_\n",
    "    print(svm_gs.best_params_)\n",
    "    print('SVM: {}'.format(svm_best.score(X_test, Y_test)))\n",
    "    y_pred = svm_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "    \n",
    "    a_table.iloc[k,1] = metrics.accuracy_score(y_pred,Y_test)\n",
    "    a_table.iloc[k,2] = MCC\n",
    "    a_table.iloc[k,3] = precision\n",
    "    a_table.iloc[k,4] = recall\n",
    "    k=k+1\n",
    "    \n",
    "    \n",
    "    \n",
    "    filename = COLUMS[i] + \"_adaboost_best-pubchem.model\"\n",
    "    pickle.dump(adaboost_best, open(filename, 'wb'))\n",
    "\n",
    "    filename = COLUMS[i] + \"knn_best-pubchem.model\"\n",
    "    pickle.dump(knn_best, open(filename, 'wb'))\n",
    "    \n",
    "    filename = COLUMS[i] + \"rf_best-pubchem.model\"\n",
    "    pickle.dump(rf_best, open(filename, 'wb'))\n",
    "\n",
    "    filename = COLUMS[i] + \"svm_best-pubchem.model\"\n",
    "    pickle.dump(svm_best, open(filename, 'wb'))    \n",
    "    \n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0.01941662, 0.01367693, 0.96690645]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[0][1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X['label'] =  data[Y].values\n",
    "X= X[X['label'] != \"inc.\"]\n",
    "X= X[X['label'] != \"n.a.\"]\n",
    "print(X.shape)d13tol1v\n",
    "    #X['label']= X['label'].apply(LabelEncoder().fit_transform)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, X['label'], test_size=0.2, random_state=50223251)\n",
    "    \n",
    "    print(Y_test.value_counts())\n",
    "    print(Y_train.value_counts())\n",
    "    filename = Y + \".xlsx\"\n",
    "    with pd.ExcelWriter(filename) as writer:  \n",
    "        X_train.to_excel(writer, sheet_name='X_train')\n",
    "        X_test.to_excel(writer,sheet_name='X_test')\n",
    "        Y_train.to_excel(writer, sheet_name='Y_train')\n",
    "        Y_test.to_excel(writer, sheet_name='Y_test')\n",
    "    X_train =X_train.filter(regex='Bit').values\n",
    "    X_test =X_test.filter(regex='Bit').values\n",
    "    Y_train = Y_train\n",
    "    Y_test = Y_test\n",
    "\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "    #encoder.fit(X['label'])\n",
    "    #Y_val = encoder.transform(Y)\n",
    "    # encoding train labels \n",
    "    encoder.fit(Y_train)\n",
    "    Y_train = encoder.transform(Y_train)\n",
    "\n",
    "    # encoding test labels \n",
    "    #encoder.fit(Y_test)\n",
    "    Y_test = encoder.transform(Y_test)\n",
    "\n",
    "    y_true = Y_test\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    print(\"##################- Running KNN - ######################\")\n",
    "    knn = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "    params_knn = {'n_neighbors': np.arange(1, 50),\n",
    "                  'algorithm' :['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                  'weights': ['uniform','distance'],\n",
    "                  'metric': ['euclidean','manhattan']}#use gridsearch to test all values for n_neighbors\n",
    "    knn_gs = GridSearchCV(knn, params_knn, cv=skf, n_jobs=-1, scoring=\"f1_micro\")#fit model to training data\n",
    "    knn_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    knn_best = knn_gs.best_estimator_#check best n_neigbors value\n",
    "    print(knn_gs.best_params_)\n",
    "    print('knn: {}'.format(knn_best.score(X_test, Y_test)))\n",
    "    y_pred = knn_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    print(\"##################- Running AdaBoostClassifier - ######################\")\n",
    "\n",
    "    adaboost = AdaBoostClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "    params_adaboost = {'n_estimators': np.arange(1, 500),}#use gridsearch to test all values for n_neighbors\n",
    "    adaboost_gs = GridSearchCV(adaboost, params_adaboost, cv=skf, n_jobs=-1,scoring= \"f1_micro\")#fit model to training data\n",
    "    adaboost_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    adaboost_best = adaboost_gs.best_estimator_#check best n_neigbors value\n",
    "    print(adaboost_gs.best_params_)\n",
    "    print('adaboost: {}'.format(adaboost_best.score(X_test, Y_test)))\n",
    "    y_pred = adaboost_gs.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    print(\"##################- Running RandomForestClassifier - ######################\")\n",
    "\n",
    "    rf = RandomForestClassifier(class_weight='balanced')#create a dictionary of all values we want to test for n_estimators\n",
    "    params_rf = {'n_estimators': [50, 100, 200,300,500, 1000], \n",
    "                 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                 'min_samples_split': [3, 5, 10],\n",
    "                 'min_samples_leaf': [8, 10, 12],\n",
    "                 'max_depth': [80, 90, 100, 110],\n",
    "                 'max_features': [3, 5, 10, 20],\n",
    "                 'bootstrap': [True],}#use gridsearch to test all values for n_estimators\n",
    "    rf_gs = GridSearchCV(rf, params_rf, cv=skf,n_jobs=-1,scoring= \"f1_micro\")#fit model to training data\n",
    "    rf_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    rf_best = rf_gs.best_estimator_#check best n_estimators value\n",
    "    print(rf_gs.best_params_)\n",
    "    print('rf: {}'.format(rf_best.score(X_test, Y_test)))\n",
    "    y_pred = rf_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "    C_range=[1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]\n",
    "    gamma_range=[1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,1.e-01, 1.e+00, ]\n",
    "\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma_range,'C': C_range},\n",
    "                        {'kernel': ['poly'], 'gamma': gamma_range,'C': C_range},\n",
    "                        {'kernel': ['sigmoid'],'gamma': gamma_range, 'C': C_range}]\n",
    "    print(\"##################- Running SVM - ######################\")\n",
    "\n",
    "    svm_gs = GridSearchCV(SVC(class_weight='balanced',probability=True), tuned_parameters,iid=True, cv=skf,scoring=\"f1_micro\",n_jobs=-1)\n",
    "    svm_gs.fit(X_train, Y_train)\n",
    "    svm_best = svm_gs.best_estimator_\n",
    "    print(svm_gs.best_params_)\n",
    "    print('SVM: {}'.format(svm_best.score(X_test, Y_test)))\n",
    "    y_pred = svm_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "  \n",
    "\n",
    "    \n",
    "    filename = Y + \"_adaboost_best.model\"\n",
    "    pickle.dump(adaboost_best, open(filename, 'wb'))\n",
    "\n",
    "    filename = Y + \"knn_best.model\"\n",
    "    pickle.dump(knn_best, open(filename, 'wb'))\n",
    "    \n",
    "    filename = Y + \"rf_best.model\"\n",
    "    pickle.dump(rf_best, open(filename, 'wb'))\n",
    "\n",
    "    filename = Y + \"svm_best.model\"\n",
    "    pickle.dump(svm_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['AGO_RXR', 'ANT_RXR', 'BIN_RXR', 'AGO_GR',\n",
    "       'ANT_GR', 'BIN_GR', 'AGO_AR', 'ANT_AR', 'BIN_AR', 'AGO_ERA', 'ANT_ERA',\n",
    "       'BIN_ERA', 'AGO_ERB', 'ANT_ERB', 'BIN_ERB', 'AGO_FXR', 'ANT_FXR',\n",
    "       'BIN_FXR', 'AGO_PPARD', 'ANT_PPARD', 'BIN_PPARD', 'AGO_PPARG',\n",
    "       'ANT_PPARG', 'BIN_PPARG', 'AGO_PPARA', 'ANT_PPARA', 'BIN_PPARA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGO_RXR = data['NURA_ID','AGO_RXR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Y in cols:\n",
    "    print(\"################## -- Running Calculations for \", Y,\"--###################\")\n",
    "    X = df_morgan\n",
    "    print(data[Y].value_counts())\n",
    "    print(X.shape)\n",
    "    X['label'] =  data[Y].values\n",
    "    X= X[X['label'] != \"inc.\"]\n",
    "    X= X[X['label'] != \"n.a.\"]\n",
    "    print(X['label'].value_counts())\n",
    "    print(X.shape)\n",
    "    #X['label']= X['label'].apply(LabelEncoder().fit_transform)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, X['label'], test_size=0.2, random_state=50223251)\n",
    "    \n",
    "    \n",
    "    #print(X['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def function(Y):\n",
    "    print(\"################## -- Running Calculations for \", Y,\"--###################\")\n",
    "    X = df_morgan\n",
    "    print(X.shape)\n",
    "    X['label'] =  data[Y].values\n",
    "    X= X[X['label'] != \"inc.\"]\n",
    "    X= X[X['label'] != \"n.a.\"]\n",
    "    print(X.shape)\n",
    "    #X['label']= X['label'].apply(LabelEncoder().fit_transform)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, X['label'], test_size=0.2, random_state=50223251)\n",
    "    \n",
    "    print(Y_test.value_counts())\n",
    "    print(Y_train.value_counts())\n",
    "    filename = Y + \".xlsx\"\n",
    "    with pd.ExcelWriter(filename) as writer:  \n",
    "        X_train.to_excel(writer, sheet_name='X_train')\n",
    "        X_test.to_excel(writer,sheet_name='X_test')\n",
    "        Y_train.to_excel(writer, sheet_name='Y_train')\n",
    "        Y_test.to_excel(writer, sheet_name='Y_test')\n",
    "    X_train =X_train.filter(regex='Bit').values\n",
    "    X_test =X_test.filter(regex='Bit').values\n",
    "    Y_train = Y_train\n",
    "    Y_test = Y_test\n",
    "\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "    #encoder.fit(X['label'])\n",
    "    #Y_val = encoder.transform(Y)\n",
    "    # encoding train labels \n",
    "    encoder.fit(Y_train)\n",
    "    Y_train = encoder.transform(Y_train)\n",
    "\n",
    "    # encoding test labels \n",
    "    #encoder.fit(Y_test)\n",
    "    Y_test = encoder.transform(Y_test)\n",
    "\n",
    "    y_true = Y_test\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    print(\"##################- Running KNN - ######################\")\n",
    "    knn = KNeighborsClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "    params_knn = {'n_neighbors': np.arange(1, 50),\n",
    "                  'algorithm' :['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                  'weights': ['uniform','distance'],\n",
    "                  'metric': ['euclidean','manhattan']}#use gridsearch to test all values for n_neighbors\n",
    "    knn_gs = GridSearchCV(knn, params_knn, cv=skf, n_jobs=-1, scoring=\"f1_micro\")#fit model to training data\n",
    "    knn_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    knn_best = knn_gs.best_estimator_#check best n_neigbors value\n",
    "    print(knn_gs.best_params_)\n",
    "    print('knn: {}'.format(knn_best.score(X_test, Y_test)))\n",
    "    y_pred = knn_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    print(\"##################- Running AdaBoostClassifier - ######################\")\n",
    "\n",
    "    adaboost = AdaBoostClassifier()#create a dictionary of all values we want to test for n_neighbors\n",
    "    params_adaboost = {'n_estimators': np.arange(1, 500),}#use gridsearch to test all values for n_neighbors\n",
    "    adaboost_gs = GridSearchCV(adaboost, params_adaboost, cv=skf, n_jobs=-1,scoring= \"f1_micro\")#fit model to training data\n",
    "    adaboost_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    adaboost_best = adaboost_gs.best_estimator_#check best n_neigbors value\n",
    "    print(adaboost_gs.best_params_)\n",
    "    print('adaboost: {}'.format(adaboost_best.score(X_test, Y_test)))\n",
    "    y_pred = adaboost_gs.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    print(\"##################- Running RandomForestClassifier - ######################\")\n",
    "\n",
    "    rf = RandomForestClassifier(class_weight='balanced')#create a dictionary of all values we want to test for n_estimators\n",
    "    params_rf = {'n_estimators': [50, 100, 200,300,500, 1000], \n",
    "                 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                 'min_samples_split': [3, 5, 10],\n",
    "                 'min_samples_leaf': [8, 10, 12],\n",
    "                 'max_depth': [80, 90, 100, 110],\n",
    "                 'max_features': [3, 5, 10, 20],\n",
    "                 'bootstrap': [True],}#use gridsearch to test all values for n_estimators\n",
    "    rf_gs = GridSearchCV(rf, params_rf, cv=skf,n_jobs=-1,scoring= \"f1_micro\")#fit model to training data\n",
    "    rf_gs.fit(X_train, Y_train)\n",
    "    #save best model\n",
    "    rf_best = rf_gs.best_estimator_#check best n_estimators value\n",
    "    print(rf_gs.best_params_)\n",
    "    print('rf: {}'.format(rf_best.score(X_test, Y_test)))\n",
    "    y_pred = rf_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "    C_range=[1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]\n",
    "    gamma_range=[1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,1.e-01, 1.e+00, ]\n",
    "\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': gamma_range,'C': C_range},\n",
    "                        {'kernel': ['poly'], 'gamma': gamma_range,'C': C_range},\n",
    "                        {'kernel': ['sigmoid'],'gamma': gamma_range, 'C': C_range}]\n",
    "    print(\"##################- Running SVM - ######################\")\n",
    "\n",
    "    svm_gs = GridSearchCV(SVC(class_weight='balanced',probability=True), tuned_parameters,iid=True, cv=skf,scoring=\"f1_micro\",n_jobs=-1)\n",
    "    svm_gs.fit(X_train, Y_train)\n",
    "    svm_best = svm_gs.best_estimator_\n",
    "    print(svm_gs.best_params_)\n",
    "    print('SVM: {}'.format(svm_best.score(X_test, Y_test)))\n",
    "    y_pred = svm_best.predict(X_test)\n",
    "    print(\"ACC\", accuracy_score(y_true,y_pred))\n",
    "    print(\"recall\",recall_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"precision\",precision_score(y_true, y_pred,average='weighted'))\n",
    "    print(\"MCC\", metrics.matthews_corrcoef(y_true, y_pred))\n",
    "  \n",
    "\n",
    "    \n",
    "    filename = Y + \"_adaboost_best.model\"\n",
    "    pickle.dump(adaboost_best, open(filename, 'wb'))\n",
    "\n",
    "    filename = Y + \"knn_best.model\"\n",
    "    pickle.dump(knn_best, open(filename, 'wb'))\n",
    "    \n",
    "    filename = Y + \"rf_best.model\"\n",
    "    pickle.dump(rf_best, open(filename, 'wb'))\n",
    "\n",
    "    filename = Y + \"svm_best.model\"\n",
    "    pickle.dump(svm_best, open(filename, 'wb'))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for Y in cols:   \n",
    "    function(Y)\n",
    "#threads = []\n",
    "#for Y in cols:\n",
    " #   t = threading.Thread(target=function(Y))\n",
    " #   threads.append(t)\n",
    " #   t.start()\n",
    " #   t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0.01979932, 0.01606667, 0.96413401],\n",
    "     [0.02424475, 0.01255782, 0.96319743]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
